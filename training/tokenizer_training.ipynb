{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hREq_EMaHdLg",
    "outputId": "6e9573f9-3373-4caf-d770-bf74462dc4c5"
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8I1U0HadHdLh",
    "outputId": "73be700c-5647-41c5-eb3b-8f05b0cb56bf"
   },
   "outputs": [],
   "source": [
    "#!pip install triton transformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugxLjpR-HdLh",
    "outputId": "e7ba654a-b74b-4064-c4c6-ce5761ba075c"
   },
   "outputs": [],
   "source": [
    "!pip install -q tiktoken datasets==2.14.5 omegaconf \n",
    "#deepspeed accelerate==0.23 bitsandbytes einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lc9ve2HHdLi",
    "outputId": "b694039a-2279-47d8-dd5c-ae9daae710a1"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade huggingface_hub\n",
    "!pip install -U -q \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2epkrQlmHdLj",
    "outputId": "95fdd855-1aea-4a46-be36-0caee0279a9d"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login  --token \"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\" #Read token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Ss9AS5HdLj"
   },
   "source": [
    "## Finetuning Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3-w18ZQHdLl"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "HF_WRITE_TOKEN = \"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\"\n",
    "HF_READ_TOKEN = \"\"\n",
    "\n",
    "api = HfApi(token=HF_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y52e-IYHdLo"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "project_root =  os.path.dirname(current_dir)\n",
    "\n",
    "config_path = os.path.join(project_root, \"config\", \"config.yaml\")\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# repo_names = config.data.datasets # For main tokenizer training\n",
    "repo_names = [\"Aletheia-ng/pretrain_test\"]\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "    dataset = load_dataset(repo_name, token=HF_READ_TOKEN)\n",
    "    all_datasets.append(dataset)\n",
    "\n",
    "# Concatenate all datasets\n",
    "dataset = concatenate_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "def get_training_corpus1():\n",
    "    return (\n",
    "        dataset[\"train\"][i : i + batch_size][\"text\"]\n",
    "        for i in range(0, len(dataset[\"train\"]), batch_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(train_dataset), batch_size):\n",
    "        samples = train_dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tokenizer you want (sentencepiece tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "\n",
    "# tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 64000 , new_special_tokens=[\"<classify>\", \"<translate>\", \"<eng>\",\n",
    "\"<yor>\",\"<ibo>\", \"<hau>\",\"<sentiment>\", \"<topic>\",\"<headline>\", \"<ner>\", \"<correct>\", \"<clean>\", \"<title>\",\"|end_of_text|\", \"<summarize>\",\"<summary>\",\n",
    "\"<clean>\",\"<correct>\",\"<diacritize>\",\"<NER>\",\"<tag>\",\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-DATE\", \"I-DATE\",\n",
    "\"<qa>\",\"<question>\",\"<answer>\",\"<context>\", \"<classify>\",\"<sentiment>\",\"<topic>\",\"<lang_ID>\",\"<lang_ID_label>\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained(\"sabiyarn-tokenizer\")\n",
    "\n",
    "# Change the repo to a temporary one if you are testing the model\n",
    "# new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarnV2\", token=HF_WRITE_TOKEN)\n",
    "\n",
    "new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarn_test\", token=HF_WRITE_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and \\\n",
    "positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer \\\n",
    "to a question, please don't share false information.<</SYS>> \\\n",
    "{}[/INST]\"\n",
    "\n",
    "prompt = prompt_template.format(\" who is elon musk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "tokenizer_path = \"sabiyarn-tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n",
    "\n",
    "\n",
    "prompt = \"Describe a painting of a falcon in a very detailed way.\" # Change this to your prompt\n",
    "prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(\"Length of processed tokens: \", len(tokens[0]))\n",
    "print(\"Decoded tokens: \", tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the Number of Pretraining Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "\n",
    "gpt_used = \"gpt-4.1\"\n",
    "n_tokens = []\n",
    "n_sentences = []\n",
    "sabiyarn_n_tokens = []\n",
    "sabiyarn_parent_n_tokens = []\n",
    " \n",
    "for each in dataset[\"train\"]:\n",
    "    #print(each)\n",
    "    n_tokens.append(num_tokens_from_string(str(each[\"text\"]), gpt_used))\n",
    "    sabiyarn_n_tokens.append(len(new_tokenizer.encode(str(each[\"text\"]))))\n",
    "    sabiyarn_parent_n_tokens.append(len(tokenizer.encode(str(each['text']))))\n",
    "    n_sentences.append(len(sent_tokenize(str(each['text']))))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({f\"{gpt_used} tokenizer\": n_tokens, \"sabiyarn tokenizer\": sabiyarn_n_tokens, \"parent tokenizer\": sabiyarn_parent_n_tokens, \n",
    "                   \"number of sentences\": n_sentences})\n",
    "\n",
    "df.to_csv(\"pretraining_data_sample_token_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('pretraining_data_sample_token_length.csv')\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
