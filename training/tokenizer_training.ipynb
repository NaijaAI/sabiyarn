{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hREq_EMaHdLg",
    "outputId": "6e9573f9-3373-4caf-d770-bf74462dc4c5"
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8I1U0HadHdLh",
    "outputId": "73be700c-5647-41c5-eb3b-8f05b0cb56bf"
   },
   "outputs": [],
   "source": [
    "#!pip install triton transformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugxLjpR-HdLh",
    "outputId": "e7ba654a-b74b-4064-c4c6-ce5761ba075c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tiktoken datasets==2.14.5 omegaconf \n",
    "#deepspeed accelerate==0.23 bitsandbytes einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lc9ve2HHdLi",
    "outputId": "b694039a-2279-47d8-dd5c-ae9daae710a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade huggingface_hub\n",
    "!pip install -U -q \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2epkrQlmHdLj",
    "outputId": "95fdd855-1aea-4a46-be36-0caee0279a9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: #Read token\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login  --token \"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\" #Read token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Ss9AS5HdLj"
   },
   "source": [
    "## Finetuning Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "I3-w18ZQHdLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "HF_WRITE_TOKEN = \"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\"\n",
    "HF_READ_TOKEN = \"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\"\n",
    "\n",
    "api = HfApi(token=HF_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2y52e-IYHdLo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 32/32 [00:00<00:00, 39.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "config_path = \"../config/config.yaml\" #os.path.join(project_root, \"config\", \"config.yaml\")\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# repo_names = config.data.datasets # For main tokenizer training\n",
    "repo_names = [\"Aletheia-ng/pretrain_test\"]\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "    dataset = load_dataset(repo_name, token=HF_READ_TOKEN)\n",
    "    all_datasets.append(dataset[\"train\"])\n",
    "\n",
    "# Concatenate all datasets\n",
    "dataset = concatenate_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "def get_training_corpus1():\n",
    "    return (\n",
    "        dataset[i : i + batch_size][\"text\"]\n",
    "        for i in range(0, len(dataset[\"train\"]), batch_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    train_dataset = dataset\n",
    "    for start_idx in range(0, len(train_dataset), batch_size):\n",
    "        samples = train_dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tokenizer you want (sentencepiece tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), '<unk>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BloomTokenizerFast, GemmaTokenizerFast, LlamaTokenizerFast\n",
    "\n",
    "# tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=HF_READ_TOKEN) #I am not certain LLama uses sentence piece\n",
    "\n",
    "#if it doesnt, fetch all special tokens (most open source datasets have llama's structure)\n",
    "all_special_tokens = tokenizer.all_special_tokens_extended\n",
    "print(all_special_tokens)\n",
    "\n",
    "# Comment out if llama uses sentencepiece indeed\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-2b\", token= HF_READ_TOKEN) # Uses sentence piece, tokenizerFast for fast batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '<unk>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all llama special tokens\n",
    "#all_special_tokens +\n",
    "special_tokens =  [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-DATE\", \"I-DATE\",\n",
    "    \"<context>\", \"<lug>\", \"<qa>\", \"<tsn>\", \"<afr>\", \"<identify>\", \"<fuv>\", \"<reason>\", \"<din>\", \"<xsm>\", \"<zu>\", \"<tmh>\", \n",
    "    \"<ti>\", \"<tzm>\", \"<ny>\", \"<arb>\", \"<correct>\", \"<dyu>\", \"<eng>\", \"<kea>\", \"<fra>\", \"<score>\", \"<kab>\", \"<amh>\", \"<toxic>\", \"<pcm>\",\n",
    "    \"<response>\", \"<hau>\", \"<swh>\", \"<snq>\", \"<translate>\", \"<ful>\", \"<prompt>\", \"<ton>\", \"<vag>\", \"<nup>\", \"<kmb>\", \"<mey>\", \"<classify>\",\n",
    "    \"<text>\", \"<luo>\", \"<sn>\", \"<nus>\", \"<ven>\", \"<oke>\", \"<yor>\", \"<sentiment>\", \"<headline>\", \"<xh>\", \"<son>\", \"<igl>\", \"<kik>\", \n",
    "    \"<clean>\", \"<wolof>\", \"<sag>\", \"<aku>\", \"<tso>\", \"<ewe>\", \"<ngl>\", \"<run>\", \"<lang_ID>\", \"<gah>\", \"<bm>\", \"<kbp>\", \"<STR>\", \"<umb>\",\n",
    "    \"<aka>\", \"<lin>\", \"<urh>\", \"<tum>\", \"<nso>\", \"<ssw>\", \"<topic>\", \"<title>\", \"<fat>\", \"<som>\", \"<vai>\", \"<tag>\", \"<sot>\", \"<mos>\", \n",
    "    \"<tiv>\", \"<kon>\", \"<diacritize>\", \"<fon>\", \"<twi>\", \"<nde>\", \"<NER>\", \"<bem>\", \"<knc>\", \"<nya>\", \"<orm>\", \"<oro>\", \"<mlg>\", \"<shi>\", \"<|end_of_text|>\",\n",
    "    \"<summarize>\", \"<lus>\", \"<gaa>\", \"<ibo>\", \"<ibb>\", \"<kin>\", \"<mzw>\", \"<kam>\",\"<question>\", \"<summary>\", \"<answer>\",\"<ner>\", \"<intent>\", \"<lang_ID_label>\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 64000 , new_special_tokens=special_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained(\"sabiyarn-tokenizer\")\n",
    "\n",
    "# Change the repo to a temporary one if you are testing the model\n",
    "# new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarnV2\", token=HF_WRITE_TOKEN)\n",
    "\n",
    "new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarn_test\", token=HF_WRITE_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and \\\n",
    "positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer \\\n",
    "to a question, please don't share false information.<</SYS>> \\\n",
    "{}[/INST]\"\n",
    "\n",
    "prompt = prompt_template.format(\" who is elon musk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "tokenizer_path = \"sabiyarn-tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n",
    "\n",
    "\n",
    "prompt = \"Describe a painting of a falcon in a very detailed way.\" # Change this to your prompt\n",
    "prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(\"Length of processed tokens: \", len(tokens[0]))\n",
    "print(\"Decoded tokens: \", tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the Number of Pretraining Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "\n",
    "gpt_used = \"gpt-4.1\"\n",
    "n_tokens = []\n",
    "n_sentences = []\n",
    "sabiyarn_n_tokens = []\n",
    "sabiyarn_parent_n_tokens = []\n",
    " \n",
    "for each in dataset[\"train\"]:\n",
    "    #print(each)\n",
    "    n_tokens.append(num_tokens_from_string(str(each[\"text\"]), gpt_used))\n",
    "    sabiyarn_n_tokens.append(len(new_tokenizer.encode(str(each[\"text\"]))))\n",
    "    sabiyarn_parent_n_tokens.append(len(tokenizer.encode(str(each['text']))))\n",
    "    n_sentences.append(len(sent_tokenize(str(each['text']))))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({f\"{gpt_used} tokenizer\": n_tokens, \"sabiyarn tokenizer\": sabiyarn_n_tokens, \"parent tokenizer\": sabiyarn_parent_n_tokens, \n",
    "                   \"number of sentences\": n_sentences})\n",
    "\n",
    "df.to_csv(\"pretraining_data_sample_token_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('pretraining_data_sample_token_length.csv')\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
