{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hREq_EMaHdLg",
    "outputId": "6e9573f9-3373-4caf-d770-bf74462dc4c5"
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8I1U0HadHdLh",
    "outputId": "73be700c-5647-41c5-eb3b-8f05b0cb56bf"
   },
   "outputs": [],
   "source": [
    "#!pip install triton transformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugxLjpR-HdLh",
    "outputId": "e7ba654a-b74b-4064-c4c6-ce5761ba075c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tiktoken datasets==2.14.5 omegaconf \n",
    "#deepspeed accelerate==0.23 bitsandbytes einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lc9ve2HHdLi",
    "outputId": "b694039a-2279-47d8-dd5c-ae9daae710a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade huggingface_hub\n",
    "!pip install -U -q \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2epkrQlmHdLj",
    "outputId": "95fdd855-1aea-4a46-be36-0caee0279a9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: #Read token\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login  --token \"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\" #Read token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Ss9AS5HdLj"
   },
   "source": [
    "## Finetuning Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I3-w18ZQHdLl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "HF_WRITE_TOKEN = \"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\"\n",
    "HF_READ_TOKEN = \"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\"\n",
    "\n",
    "api = HfApi(token=HF_WRITE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2y52e-IYHdLo"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'Aletheia-ng/pretrain_test' on the Hub (ConnectionError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m all_datasets = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m repo_name \u001b[38;5;129;01min\u001b[39;00m repo_names:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHF_READ_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     all_datasets.append(dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Concatenate all datasets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:2129\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   2124\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2125\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2126\u001b[39m )\n\u001b[32m   2128\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1815\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1813\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1814\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1824\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1512\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[39m\n\u001b[32m   1507\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1508\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1509\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1510\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1511\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1516\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\load.py:1468\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# noqa catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[32m   1460\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1461\u001b[39m         e,\n\u001b[32m   1462\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1466\u001b[39m         ),\n\u001b[32m   1467\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m404\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[32m   1470\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hub\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mConnectionError\u001b[39m: Couldn't reach 'Aletheia-ng/pretrain_test' on the Hub (ConnectionError)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "config_path = \"../config/config.yaml\" #os.path.join(project_root, \"config\", \"config.yaml\")\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# repo_names = config.data.datasets # For main tokenizer training\n",
    "repo_names = [\"Aletheia-ng/pretrain_test\"]\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "    dataset = load_dataset(repo_name, token=HF_READ_TOKEN)\n",
    "    all_datasets.append(dataset[\"train\"])\n",
    "\n",
    "# Concatenate all datasets\n",
    "dataset = concatenate_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "def get_training_corpus1():\n",
    "    return (\n",
    "        dataset[i : i + batch_size][\"text\"]\n",
    "        for i in range(0, len(dataset[\"train\"]), batch_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    train_dataset = dataset\n",
    "    for start_idx in range(0, len(train_dataset), batch_size):\n",
    "        samples = train_dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tokenizer you want (sentencepiece tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), '<unk>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BloomTokenizerFast, GemmaTokenizerFast, LlamaTokenizerFast\n",
    "\n",
    "# tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=HF_READ_TOKEN) #I am not certain LLama uses sentence piece\n",
    "\n",
    "#if it doesnt, fetch all special tokens (most open source datasets have llama's structure)\n",
    "all_special_tokens = tokenizer.all_special_tokens_extended\n",
    "print(all_special_tokens)\n",
    "\n",
    "# Comment out if llama uses sentencepiece indeed\n",
    "# tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-2b\", token= HF_READ_TOKEN) # Uses sentence piece, tokenizerFast for fast batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " '<unk>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all llama special tokens\n",
    "#all_special_tokens +\n",
    "special_tokens =  [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-DATE\", \"I-DATE\",\n",
    "    \"<context>\", \"<lug>\", \"<qa>\", \"<tsn>\", \"<afr>\", \"<identify>\", \"<fuv>\", \"<reason>\", \"<din>\", \"<xsm>\", \"<zu>\", \"<tmh>\", \n",
    "    \"<ti>\", \"<tzm>\", \"<ny>\", \"<arb>\", \"<correct>\", \"<dyu>\", \"<eng>\", \"<kea>\", \"<fra>\", \"<score>\", \"<kab>\", \"<amh>\", \"<toxic>\", \"<pcm>\",\n",
    "    \"<response>\", \"<hau>\", \"<swh>\", \"<snq>\", \"<translate>\", \"<ful>\", \"<prompt>\", \"<ton>\", \"<vag>\", \"<nup>\", \"<kmb>\", \"<mey>\", \"<classify>\",\n",
    "    \"<text>\", \"<luo>\", \"<sn>\", \"<nus>\", \"<ven>\", \"<oke>\", \"<yor>\", \"<sentiment>\", \"<headline>\", \"<xh>\", \"<son>\", \"<igl>\", \"<kik>\", \n",
    "    \"<clean>\", \"<wolof>\", \"<sag>\", \"<aku>\", \"<tso>\", \"<ewe>\", \"<ngl>\", \"<run>\", \"<lang_ID>\", \"<gah>\", \"<bm>\", \"<kbp>\", \"<STR>\", \"<umb>\",\n",
    "    \"<aka>\", \"<lin>\", \"<urh>\", \"<tum>\", \"<nso>\", \"<ssw>\", \"<topic>\", \"<title>\", \"<fat>\", \"<som>\", \"<vai>\", \"<tag>\", \"<sot>\", \"<mos>\", \n",
    "    \"<tiv>\", \"<kon>\", \"<diacritize>\", \"<fon>\", \"<twi>\", \"<nde>\", \"<NER>\", \"<bem>\", \"<knc>\", \"<nya>\", \"<orm>\", \"<oro>\", \"<mlg>\", \"<shi>\", \"<|end_of_text|>\",\n",
    "    \"<summarize>\", \"<lus>\", \"<gaa>\", \"<ibo>\", \"<ibb>\", \"<kin>\", \"<mzw>\", \"<kam>\",\"<question>\", \"<summary>\", \"<answer>\",\"<ner>\", \"<intent>\", \"<lang_ID_label>\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 64000 , new_special_tokens=special_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained(\"sabiyarn-tokenizer\")\n",
    "\n",
    "# Change the repo to a temporary one if you are testing the model\n",
    "# new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarnV2\", token=HF_WRITE_TOKEN)\n",
    "\n",
    "new_tokenizer.push_to_hub(\"Aletheia-ng/SabiYarn_test\", token=HF_WRITE_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and \\\n",
    "positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer \\\n",
    "to a question, please don't share false information.<</SYS>> \\\n",
    "{}[/INST]\"\n",
    "\n",
    "prompt = prompt_template.format(\" who is elon musk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "tokenizer_path = \"sabiyarn-tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n",
    "\n",
    "\n",
    "prompt = \"Describe a painting of a falcon in a very detailed way.\" # Change this to your prompt\n",
    "prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(\"Length of processed tokens: \", len(tokens[0]))\n",
    "print(\"Decoded tokens: \", tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the Number of Pretraining Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "\n",
    "gpt_used = \"gpt-4.1\"\n",
    "n_tokens = []\n",
    "n_sentences = []\n",
    "sabiyarn_n_tokens = []\n",
    "sabiyarn_parent_n_tokens = []\n",
    " \n",
    "for each in dataset[\"train\"]:\n",
    "    #print(each)\n",
    "    n_tokens.append(num_tokens_from_string(str(each[\"text\"]), gpt_used))\n",
    "    sabiyarn_n_tokens.append(len(new_tokenizer.encode(str(each[\"text\"]))))\n",
    "    sabiyarn_parent_n_tokens.append(len(tokenizer.encode(str(each['text']))))\n",
    "    n_sentences.append(len(sent_tokenize(str(each['text']))))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({f\"{gpt_used} tokenizer\": n_tokens, \"sabiyarn tokenizer\": sabiyarn_n_tokens, \"parent tokenizer\": sabiyarn_parent_n_tokens, \n",
    "                   \"number of sentences\": n_sentences})\n",
    "\n",
    "df.to_csv(\"pretraining_data_sample_token_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('pretraining_data_sample_token_length.csv')\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
