{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66fb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_id_token = 26290\n",
    "# lang_id_label_token = 102\n",
    "# classify_token = 42303\n",
    "# sentiment_token = 13896\n",
    "# topic_token = 49704\n",
    "# ner_token = 30827\n",
    "# qa_token = 23846\n",
    "# answer_token = 47682\n",
    "# tag_token = 7203\n",
    "# diacritize_token = 19588\n",
    "# correct_token = 48220\n",
    "# clean_token = 31666\n",
    "# summarize_token = 114\n",
    "# summary_token = 16150\n",
    "# title_token = 27246\n",
    "# headline_token = 49296\n",
    "# end_of_token = 14004\n",
    "# translate_token = 34635\n",
    "# # classify_token = 5\n",
    "# ner_token = 30827\n",
    "# eng = 1516\n",
    "# yor = 40754\n",
    "# ibo = 26762\n",
    "# hau = 13679\n",
    "# pcm = 28112\n",
    "# prompt = 37160\n",
    "# response = 2474\n",
    "# MASK = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bf7c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e802599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for triton\n"
     ]
    }
   ],
   "source": [
    "!pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a6d3b1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msabiyarn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m vocab_size = \u001b[32m64000\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Test 8: Distributed Training Config\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\Desktop\\AI work\\AI projects\\sabiyarn\\sabiyarn\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelArgs, SabiYarn\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Optional imports that might have dependencies\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\Desktop\\AI work\\AI projects\\sabiyarn\\sabiyarn\\model.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory_reasoning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogicNetwork\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdifferential_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiffAttention, DiffAttnArgs\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mMLA\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MLA, MLAConfig, ColumnParallelLinear, RowParallelLinear, linear\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mMHA\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelfAttention, SelfAttnArgs, precompute_freqs_cis\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAttentionType\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m, Enum):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\Desktop\\AI work\\AI projects\\sabiyarn\\sabiyarn\\MLA.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, Optional, Literal\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m act_quant, weight_dequant, fp8_gemm\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m     13\u001b[39m world_size = \u001b[32m1\u001b[39m \u001b[38;5;66;03m# the number of GPUs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jeffrey Paul\\Desktop\\AI work\\AI projects\\sabiyarn\\sabiyarn\\kernel.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtl\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sabiyarn.model import *\n",
    "\n",
    "vocab_size = 64000\n",
    "# Test 8: Distributed Training Config\n",
    "def load_model():       \n",
    "    mla_config = MLAConfig(\n",
    "        hidden_size=256, num_heads=8, max_seq_len=32, max_batch_size=2,\n",
    "        attention_dropout=0.0, q_lora_rank=64, qk_rope_head_dim=16,\n",
    "        kv_lora_rank=32, v_head_dim=32, qk_nope_head_dim=16,\n",
    "        attention_bias=False, original_seq_len=32, rope_theta=10000.0,\n",
    "        rope_factor=1, beta_fast=32, beta_slow=1, mscale=1.\n",
    "    )\n",
    "    config_mla = ModelArgs(\n",
    "        dim=256, n_layers=1, n_heads=8, vocab_size=vocab_size,\n",
    "        attention_type=AttentionType.MLA, mla_config=mla_config,\n",
    "        auto_detect_distributed=True\n",
    "    )\n",
    "    model_mla = SabiYarn(config_mla)\n",
    "    model_mla.eval()\n",
    "    \n",
    "    print(\"âœ… Distributed training configuration test passed!\")\n",
    "    return model_mla\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_tokens(batch_size, seq_len, vocab_size=1000):\n",
    "    return torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tokens = generate_random_tokens(1, 2048, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        tokens = torch.randint(0, 1000, (1, 8))\n",
    "        hidden_states, logits = model(tokens, max_new_tokens=4096, temperature=1.0, top_k=None, use_multi_token=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
