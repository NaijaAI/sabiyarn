
training: 
  train_batch_size : 24
  out_dir : "out"
  eval_interval : 2000
  log_interval : 100
  eval_iters : 200
  eval_only : false  # if True, script exits right after the first eval
  always_save_checkpoint : true  # if True, always save a checkpoint after each eval
  gradient_accumulation_steps : 5 * 8  # used to simulate larger batch sizes
  decay_lr : True  # whether to decay the learning rate
  warmup_iters : 1500  # how many steps to warm up for
  lr_decay_iters : 600000  # should be ~: max_iters per Chinchilla
  min_lr : 6e-5  # minimum learning rate, should be ~: learning_rate/10 per Chinchilla
  compile : true  # use PyTorch 2.0 to compile the model to be faster
  display_model_output_iter : 768
  use_cce : True  # to use cut cross entropy or not
  scheduler: "cosine"

# wandb logging
wandb: 
  log : true  # disabled by default
  project : sabiyarn-ablations
  run_name : ablation_1  # 'run' + str(time.time())
  seed: 42

# data
data: 
  dataset : 
    - "Aletheia-ng/pretrain_data"
    - "Aletheia-ng/pretrain_data2"
    - "Aletheia-ng/pretrain_data3"
    - "Aletheia-ng/pretrain_data4"
    - "Aletheia-ng/pretrain_data5"
    - "Aletheia-ng/low_resource_languages_pretrain"
    - "Aletheia-ng/african_languages3"
  train_data_path : "data/train.bin"
  eval_data_path : "data/val.bin"

# model
model:
  vocab_size : 64000
  n_layers : 24
  n_heads : 8
  dropout : 0.0  # for pretraining 0 is good, for finetuning try 0.1+
  bias : False  # do we use bias inside LayerNorm and Linear layers?
  dim : 2048
  n_kv_heads : 4
  multiple_of : 256  # make SwiGLU hidden layer size multiple of large power of 2
  ffn_dim_multiplier : None
  norm_eps : 1e-5
  use_moe : False
  moe : None
  attention_type : "differential_attention"
  logic_network : False
  max_batch_size : 8
  max_seq_len : 1024
  use_j : True
  num_experts : 4
  num_experts_per_tok : 2
  init_from : "scratch"  # 'scratch' or 'checkpoint'
  layer_norm_epsilon: 1e-6
  max_position_embeddings: 131072
  initializer_range: 0.02
  rms_norm_eps: 1e-6
  tie_word_embeddings: false
  attention_dropout: 0.0
  tokenizer:
    name: "Aletheia-ng/SabiYarn_test"
    num_proc: 64
    process_one_file_at_a_time: True
# adamw optimizer
optimizer:
  name : "adam"
  learning_rate : 3e-4  # max learning rate
  max_iters : 600000  # total number of training iterations
  weight_decay : 1e-1
  beta1 : 0.9
  beta2 : 0.95
  grad_clip : 1.0  # clip gradients at this value, or disable if :: 0.0


# DDP settings
ddp:
  rank : 0
  backend : "nccl"
  use_dp: False

#Accelerate config
accelerate: 
  mixed_precision: "fp16"
  zero_stage: 3
  gradient_clipping: 1.0

