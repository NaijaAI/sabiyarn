#!/usr/bin/env python3
"""
Modal test runner for GitHub Actions.
This script runs the SabiYarn model tests on Modal's GPU instances.

The key difference from test_model_initialization.py is that this script
does NOT import any GPU-dependent modules at the top level, avoiding
the Triton initialization error on CPU runners.
"""

import modal
import sys
import os

# Add the project root to path so we can import sabiyarn as a package
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, project_root)

# Create Modal app with custom image that includes dependencies
image = modal.Image.debian_slim(python_version="3.10").pip_install_from_requirements("requirements.txt")

app = modal.App("sabiyarn-tests")

@app.function(gpu="A10G", timeout=800, image=image)
def run_tests_on_gpu():
    """
    Run all tests on Modal GPU instance.
    This function runs on GPU, so all GPU-dependent imports are safe here.
    """
    import sys
    import os
    import torch
    
    # Add the project root to path
    project_root = os.path.join(os.path.dirname(__file__), '..')
    sys.path.insert(0, project_root)
    
    # Now import the GPU-dependent modules (this happens on GPU)
    from sabiyarn.model import ModelArgs, SabiYarn, AttentionType
    from sabiyarn.MLA import MLAConfig
    from sabiyarn.differential_attention import DiffAttnArgs
    
    # Try to import cut_cross_entropy, skip test if not available
    try:
        from sabiyarn.cut_cross_entropy import cut_cross_entropy_loss
        CCE_AVAILABLE = True
    except ImportError:
        CCE_AVAILABLE = False
        cut_cross_entropy_loss = None
    
    def test_cut_cross_entropy():
        """Test cut cross entropy loss function."""
        if not CCE_AVAILABLE or cut_cross_entropy_loss is None:
            print("‚ö†Ô∏è Cut Cross Entropy module not available, skipping test")
            return True
        
        print("üß™ Testing Cut Cross Entropy...")
        try:
            logits = torch.randn(4, 10, 100)
            targets = torch.randint(0, 100, (4, 10))
            loss = cut_cross_entropy_loss(logits, targets)
            assert loss.item() > 0, "Loss should be positive"
            print("‚úÖ Cut Cross Entropy test passed!")
            return True
        except Exception as e:
            print(f"‚ùå Cut Cross Entropy test failed: {e}")
            return False
    
    def test_mha_model_initialization():
        """Test model initialization with Multi-Head Attention."""
        print("üß™ Testing MHA Model Initialization...")
        
        try:
            # Create MHA configuration
            config = ModelArgs(
                dim=256,
                n_layers=2,
                n_heads=8,
                n_kv_heads=4,
                vocab_size=1000,
                max_batch_size=2,
                max_seq_len=32,
                attention_type=AttentionType.SELF_ATTENTION
            )
            
            # Initialize model
            model = SabiYarn(config)
            print(f"‚úÖ MHA model created: {model.get_model_size()}")
            
            # Test forward pass
            tokens = torch.randint(0, 1000, (1, 16))
            hidden_states, logits = model(tokens, start_pos=0)
            
            print(f"   Input: {tokens.shape}")
            print(f"   Hidden states: {hidden_states.shape}")
            print(f"   Logits: {logits.shape}")
            
            expected_hidden = (1, 16, 256)
            expected_logits = (1, 16, 1000)
            
            assert hidden_states.shape == expected_hidden, f"Hidden states shape mismatch: {hidden_states.shape} vs {expected_hidden}"
            assert logits.shape == expected_logits, f"Logits shape mismatch: {logits.shape} vs {expected_logits}"
            
            print("‚úÖ MHA model test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå MHA model test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_differential_attention_model():
        """Test model initialization with Differential Attention."""
        print("\nüß™ Testing Differential Attention Model...")
        
        try:
            # Create Differential Attention configuration
            diff_args = DiffAttnArgs(
                depth=2,
                max_batch_size=2,
                n_heads=8,
                embed_dim=256,
                n_kv_heads=4,
                max_seq_len=32,
                norm_eps=1e-5
            )
            
            config = ModelArgs(
                dim=256,
                n_layers=2,
                n_heads=16,  # Total heads for transformer
                vocab_size=1000,
                max_batch_size=2,
                max_seq_len=32,
                attention_type=AttentionType.DIFFERENTIAL_ATTENTION,
                diff_attn_args=diff_args
            )
            
            # Initialize model
            model = SabiYarn(config)
            print(f"‚úÖ Differential Attention model created: {model.get_model_size()}")
            
            # Test forward pass
            tokens = torch.randint(0, 1000, (1, 16))
            hidden_states, logits = model(tokens, start_pos=0)
            
            print(f"   Input: {tokens.shape}")
            print(f"   Hidden states: {hidden_states.shape}")
            print(f"   Logits: {logits.shape}")
            
            expected_hidden = (1, 16, 256)
            expected_logits = (1, 16, 1000)
            
            assert hidden_states.shape == expected_hidden, f"Hidden states shape mismatch"
            assert logits.shape == expected_logits, f"Logits shape mismatch"
            
            print("‚úÖ Differential Attention model test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå Differential Attention model test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_mla_model():
        """Test model initialization with Multi-Head Latent Attention."""
        print("\nüß™ Testing MLA Model...")
        
        try:
            # Create MLA configuration
            mla_config = MLAConfig(
                hidden_size=256,
                num_heads=8,
                max_seq_len=32,
                max_batch_size=2,
                attention_dropout=0.0,
                q_lora_rank=64,
                qk_rope_head_dim=16,
                kv_lora_rank=32,
                v_head_dim=32,
                qk_nope_head_dim=16,
                attention_bias=False,
                original_seq_len=32,
                rope_theta=10000.0,
                rope_factor=1,
                beta_fast=32,
                beta_slow=1,
                mscale=1.
            )
            
            config = ModelArgs(
                dim=256,
                n_layers=2,
                n_heads=8,
                vocab_size=1000,
                max_batch_size=2,
                max_seq_len=32,
                attention_type=AttentionType.MLA,
                mla_config=mla_config
            )
            
            # Initialize model
            model = SabiYarn(config)
            print(f"‚úÖ MLA model created: {model.get_model_size()}")
            
            # Test forward pass
            tokens = torch.randint(0, 1000, (1, 16))
            hidden_states, logits = model(tokens, start_pos=0)
            
            print(f"   Input: {tokens.shape}")
            print(f"   Hidden states: {hidden_states.shape}")
            print(f"   Logits: {logits.shape}")
            
            expected_hidden = (1, 16, 256)
            expected_logits = (1, 16, 1000)
            
            assert hidden_states.shape == expected_hidden, f"Hidden states shape mismatch"
            assert logits.shape == expected_logits, f"Logits shape mismatch"
            
            print("‚úÖ MLA model test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå MLA model test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_mla_with_moe():
        """Test model initialization with MLA + MoE."""
        print("\nüß™ Testing MLA + MoE Model...")
        
        try:
            # Create MLA + MoE configuration
            mla_config = MLAConfig(
                hidden_size=256,
                num_heads=8,
                max_seq_len=32,
                max_batch_size=2,
                attention_dropout=0.0,
                q_lora_rank=64,
                qk_rope_head_dim=16,
                kv_lora_rank=32,
                v_head_dim=32,
                qk_nope_head_dim=16,
                attention_bias=False,
                original_seq_len=32,
                rope_theta=10000.0,
                rope_factor=1,
                beta_fast=32,
                beta_slow=1,
                mscale=1.
            )
            
            config = ModelArgs(
                dim=256,
                n_layers=2,
                n_heads=8,
                vocab_size=1000,
                max_batch_size=2,
                max_seq_len=32,
                attention_type=AttentionType.MLA,
                moe=True,
                n_routed_experts=8,
                n_activated_experts=2,
                mla_config=mla_config
            )
            
            # Initialize model
            model = SabiYarn(config)
            print(f"‚úÖ MLA + MoE model created: {model.get_model_size()}")
            
            # Test forward pass
            tokens = torch.randint(0, 1000, (1, 16))
            hidden_states, logits = model(tokens, start_pos=0)
            
            print(f"   Input: {tokens.shape}")
            print(f"   Hidden states: {hidden_states.shape}")
            print(f"   Logits: {logits.shape}")
            
            expected_hidden = (1, 16, 256)
            expected_logits = (1, 16, 1000)
            
            assert hidden_states.shape == expected_hidden, f"Hidden states shape mismatch"
            assert logits.shape == expected_logits, f"Logits shape mismatch"
            
            print("‚úÖ MLA + MoE model test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå MLA + MoE model test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_attention_factory():
        """Test the attention factory function."""
        print("\nüß™ Testing Attention Factory...")
        
        try:
            from sabiyarn.model import _create_attention
            
            # Test MHA creation
            mha_config = ModelArgs(
                dim=256,
                n_heads=8,
                attention_type=AttentionType.SELF_ATTENTION
            )
            mha_attention = _create_attention(0, mha_config)
            print("‚úÖ MHA attention module created")
            
            # Test MLA creation
            mla_config = MLAConfig(
                hidden_size=256,
                num_heads=8,
                max_seq_len=32,
                max_batch_size=2,
                attention_dropout=0.0,
                q_lora_rank=64,
                qk_rope_head_dim=16,
                kv_lora_rank=32,
                v_head_dim=32,
                qk_nope_head_dim=16,
                attention_bias=False,
                original_seq_len=32,
                rope_theta=10000.0,
                rope_factor=1,
                beta_fast=32,
                beta_slow=1,
                mscale=1.
            )
            mla_attention = _create_attention(0, mla_config)
            print("‚úÖ MLA attention module created")
            
            print("‚úÖ Attention factory test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå Attention factory test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_configuration_validation():
        """Test configuration validation."""
        print("\nüß™ Testing Configuration Validation...")
        
        try:
            from sabiyarn.model import _validate_attention_config
            
            # Test valid configurations
            valid_mha = ModelArgs(
                dim=256,
                n_heads=8,
                attention_type=AttentionType.SELF_ATTENTION
            )
            _validate_attention_config(valid_mha)
            print("‚úÖ Valid MHA config validation passed")
            
            # Test invalid MoE with wrong attention type
            try:
                invalid_moe = ModelArgs(
                    dim=256,
                    attention_type=AttentionType.SELF_ATTENTION,
                    moe=True,
                    n_routed_experts=8,
                    n_activated_experts=2
                )
                _validate_attention_config(invalid_moe)
                print("‚ùå Should have failed MoE validation")
                return False
            except ValueError:
                print("‚úÖ Invalid MoE config correctly rejected")
            
            # Test invalid expert count
            try:
                invalid_experts = ModelArgs(
                    dim=256,
                    attention_type=AttentionType.MLA,
                    moe=True,
                    n_routed_experts=4,
                    n_activated_experts=8,  # More than routed
                    mla_config=MLAConfig(
                        hidden_size=256,
                        num_heads=8,
                        max_seq_len=32,
                        max_batch_size=2,
                        attention_dropout=0.0,
                        q_lora_rank=64,
                        qk_rope_head_dim=16,
                        kv_lora_rank=32,
                        v_head_dim=32,
                        qk_nope_head_dim=16,
                        attention_bias=False,
                        original_seq_len=32,
                        rope_theta=10000.0,
                        rope_factor=1,
                        beta_fast=32,
                        beta_slow=1,
                        mscale=1.
                    )
                )
                _validate_attention_config(invalid_experts)
                print("‚ùå Should have failed expert count validation")
                return False
            except ValueError:
                print("‚úÖ Invalid expert count correctly rejected")
            
            print("‚úÖ Configuration validation test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå Configuration validation test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # Run all tests
    test_functions = [
        ("Cut Cross Entropy", test_cut_cross_entropy),
        ("MHA Model Initialization", test_mha_model_initialization),
        ("Differential Attention Model", test_differential_attention_model),
        ("MLA Model", test_mla_model),
        ("MLA + MoE Model", test_mla_with_moe),
        ("Attention Factory", test_attention_factory),
        ("Configuration Validation", test_configuration_validation),
    ]
    
    print("üöÄ **SabiYarn Model Initialization Tests (Modal GPU)**")
    print("=" * 60)
    
    passed_tests = 0
    total_tests = len(test_functions)
    
    for test_name, test_func in test_functions:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                print(f"‚úÖ {test_name}: PASSED")
                passed_tests += 1
            else:
                print(f"‚ùå {test_name}: FAILED")
        except Exception as e:
            print(f"‚ùå {test_name}: ERROR - {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print(f"üìä **Final Results: {passed_tests}/{total_tests} tests passed**")
    
    if passed_tests == total_tests:
        print("\nüéâ **All Model Initialization Tests Passed on Modal GPU!**")
        print("\n**Architecture Features Validated:**")
        print("‚úÖ Multi-Head Attention (MHA) support")
        print("‚úÖ Differential Attention support")
        print("‚úÖ Multi-Head Latent Attention (MLA) support")
        print("‚úÖ MLA + Mixture of Experts (MoE) integration")
        print("‚úÖ Attention factory pattern")
        print("‚úÖ Configuration validation")
        print("‚úÖ Unified transformer blocks")
        print("‚úÖ Modular architecture design")
        print("‚úÖ Cut Cross Entropy support")
        print("\nüèÜ **SabiYarn Model is fully functional and modular!**")
        return True
    else:
        print(f"\n‚ö†Ô∏è {total_tests - passed_tests} tests failed")
        return False

@app.local_entrypoint()
def main():
    """Entry point for Modal execution in GitHub Actions."""
    print("üîÑ Starting tests on Modal GPU...")
    result = run_tests_on_gpu.remote()
    
    if not result:
        print("‚ùå Tests failed on Modal GPU")
        sys.exit(1)
    else:
        print("‚úÖ All tests passed on Modal GPU")

if __name__ == "__main__":
    main() 