# model/tokenizer
repo_name : "Aletheia-ng/naijaweb-edu"
tokenizer_name: "BeardedMonster/SabiYarn-125M"
gradient_checkpointing: true
n_positions: 131072
sliding_window: 4096
intermediate_size: 6144
rope_theta: 10000.0
use_j: True
n_layer: 12 #28
n_head : 12
n_embd: 768 
bos_token_id: 1
eos_token_id: 2
pad_token_id: 1
n_key_value_heads: 6
use_cache: True
#activation_function: "silu"
layer_norm_epsilon: 1e-6
window: 64
dropout: 0.0 # for pretraining 0 is good, for finetuning try 0.1+
vocab_size: 52050
hidden_size: 4096
num_hidden_layers: 4
num_attention_heads: 12
num_key_value_heads: 6
hidden_act: 'silu'
max_position_embeddings: 131072
initializer_range: 0.02
rms_norm_eps: 1e-6
tie_word_embeddings: false
attention_dropout: 0.0


# dataset
dataset: "Aletheia-ng/coqa-stories"
num_proc: 128
train_data_path: "./train.bin" # CHANGE
eval_data_path: "./val.bin"
test_size: 0.2 # The fraction of the original test set to use during training (done to save compute time). It should be a value between 0 and 1.
data_version: null
max_length: 512


# train dynamics
min_lr: 3e-5 # minimum learning rate, should be ~:  learning_rate/10 per Chinchilla
weight_decay: 0.1 # increase during finetuning.
eval_every: 500
eval_steps: 105
save_every: 500
log_grads_every: 100
checkpoint: null
warmup_steps: 2500
out_dir: './train'
eval_interval: 2048
log_interval: 512
eval_iters: 500
eval_only: false # if True, script exits right after the first eval
push_to_hub_every: 768
always_save_checkpoint: true # if True, always save a checkpoint after each eval
init_from: 'scratch'
epochs: 100
gradient_accumulation_steps: 32 #128 # used to simulate larger batch sizes
atrain_batch_size: 128 # if gradient_accumulation_steps > 1, this is the micro-batch size
eval_batch_size: 128 # train_batch_size * 16
block_size: 2048
decay_lr: true # whether to decay the learning rate
warmup_iters: 2000 # how many steps to warm up for
lr_decay_iters: 2e15 # should be ~= max_iters per Chinchilla
device: "cuda"
device_type: 'cuda' 
scheduler: "cosine"

# logging
wandb_log: True # disabled by default
wandb_project: 'naija-llm'
wandb_run_name: 'mistral_run' # 'run' + str(time.time())
seed: 42

#Accelerate config
mixed_precision: "fp16"
zero_stage: 3
gradient_clipping: 1.0

# Optimizer
learning_rate: 3e-4 # max learning rate, set to about 2e-5 for finetuning.
max_iters: 2e15 # total number of training iterations
beta1: 0.9
beta2: 0.95
optimizer_name: "Adam8bit"
log_dir: "./logs"

# pytorch setting
compile: false # use PyTorch 2.0 to compile the model to be faster